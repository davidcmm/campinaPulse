{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/pandas/computation/__init__.py:25: UserWarning: The installed version of numexpr 1.4.2 is not supported in pandas and will be not be used\n",
      "The minimum supported version is 2.1\n",
      "\n",
      "  \"version is 2.1\\n\".format(ver=ver), UserWarning)\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "from sklearn import datasets\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.cross_validation import train_test_split, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.datasets import make_moons, make_circles, make_classification\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, ExtraTreesClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from sklearn.metrics import precision_recall_fscore_support, accuracy_score, f1_score, confusion_matrix\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.feature_selection import RFECV, SelectFromModel\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "#Lime \n",
    "from sklearn.pipeline import make_pipeline\n",
    "from lime.lime_tabular import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quantile Regression Results\n",
    "===========================\n",
    "\n",
    "Quantile regression is a interesting for our problem because it:\n",
    "   - Is robust to non non-normal errors and outliers\n",
    "   \n",
    "     http://fmwww.bc.edu/EC-C/S2013/823/EC823.S2013.nn04.slides.pdf     \n",
    "     http://www.econ.uiuc.edu/~roger/research/rq/QRJEP.pdf\n",
    "     \n",
    "   - Focuses on the conditional quantiles. Allows for better interpretation of effects\n",
    "     of variables. In a lot of cases, the effect is not on the conditional mean (OLS assumption)\n",
    "     \n",
    "     http://fmwww.bc.edu/EC-C/S2013/823/EC823.S2013.nn04.slides.pdf\n",
    "     http://www.econ.uiuc.edu/~roger/research/rq/QRJEP.pdf\n",
    "   \n",
    "   - Is robust to data representation (no need to normalize etc etc)\n",
    "   \n",
    "     http://www.econ.uiuc.edu/~roger/research/rq/QRJEP.pdf\n",
    "     \n",
    "Good ref for math mechanics: \n",
    "http://www.jmlr.org/papers/volume7/meinshausen06a/meinshausen06a.pdf\n",
    "\n",
    "Nice introduction for other fields: \n",
    "http://www.econ.uiuc.edu/~roger/research/rq/QReco.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quantile Regression Crash Course\n",
    "--------------------------------\n",
    "\n",
    "Plots from: \n",
    " http://www.econ.uiuc.edu/~roger/research/rq/QReco.pdf\n",
    " \n",
    " Important ref!\n",
    "\n",
    "http://ajbuckeconbikesail.net/Econ616/Quantile/JASA1999.pdf\n",
    " https://www.jstor.org/stable/2669943\n",
    " \n",
    "  (this ref provides intuiton on how to use the method, for now we can ignore the math)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**First of, what do we mean by conditional quantiles**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#from IPython.display import Image\n",
    "#Image(filename='ex1.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**First task**\n",
    "\n",
    "Find out where the variables matter (left plot)\n",
    "\n",
    "Left plot is a r-squared like value. Greater is best. But interpretation of variance does not exist.\n",
    "From this paper and others, I noticed that the ideia is less of trying to maximize this. It is a tool\n",
    "to interpret where the covariates matter.\n",
    "\n",
    "** Second task ** \n",
    "\n",
    "Look into the effect of covaritates (right plot)\n",
    "\n",
    "ps: I don't fully understand the middle plot, it is similar to the right one but another quality measure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def load_classifiers_3classes(group):\n",
    "    #Building classifiers according to best configuration per group (FALTA LINEAR!)\n",
    "    if group == 'masculino':\n",
    "        classifiers_agrad = [ ExtraTreesClassifier(bootstrap=False, class_weight=None, criterion='entropy', max_depth=None, max_features='auto', max_leaf_nodes=None, min_samples_leaf=8, min_samples_split=4, min_weight_fraction_leaf=0.0, n_estimators=20, n_jobs=-1, oob_score=False, random_state=None, verbose=0, warm_start=False), KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski', metric_params=None, n_jobs=1, n_neighbors=32, p=2, weights='uniform'), SVC(C=1, cache_size=200, class_weight=None, coef0=0.0, decision_function_shape=None, degree=3, gamma=0.25, kernel='rbf', max_iter=-1, probability=False, random_state=None, shrinking=True, tol=0.001, verbose=False), GaussianNB(), SVC(C=0.001, cache_size=200, class_weight=None, gamma='auto', kernel='linear') ]\n",
    "        classifiers_seg = [ ExtraTreesClassifier(bootstrap=False, class_weight=None, criterion='entropy', max_depth=None, max_features='auto', max_leaf_nodes=None, min_samples_leaf=8, min_samples_split=32, min_weight_fraction_leaf=0.0, n_estimators=40, n_jobs=-1, oob_score=False, random_state=None, verbose=0, warm_start=False), KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski', metric_params=None, n_jobs=1, n_neighbors=32, p=3, weights='uniform'), SVC(C=1, cache_size=200, class_weight=None, coef0=0.0, decision_function_shape=None, degree=3, gamma=0.5, kernel='rbf', max_iter=-1, probability=False, random_state=None, shrinking=True, tol=0.001, verbose=False), GaussianNB(), SVC(C=0.001, cache_size=200, class_weight=None, gamma='auto', kernel='linear') ]\n",
    "\n",
    "    elif group == 'feminino':\n",
    "        classifiers_agrad = [ ExtraTreesClassifier(bootstrap=False, class_weight=None, criterion='entropy', max_depth=None, max_features='auto', max_leaf_nodes=None, min_samples_leaf=8, min_samples_split=16, min_weight_fraction_leaf=0.0, n_estimators=60, n_jobs=-1, oob_score=False, random_state=None, verbose=0, warm_start=False), KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski', metric_params=None, n_jobs=1, n_neighbors=4, p=2, weights='uniform'), SVC(C=1, cache_size=200, class_weight=None, coef0=0.0, decision_function_shape=None, degree=3, gamma=0.25, kernel='rbf', max_iter=-1, probability=False, random_state=None, shrinking=True, tol=0.001, verbose=False), GaussianNB(), SVC(C=0.001, cache_size=200, class_weight=None, gamma='auto', kernel='linear') ]\n",
    "        classifiers_seg = [ ExtraTreesClassifier(bootstrap=False, class_weight=None, criterion='entropy', max_depth=None, max_features='auto', max_leaf_nodes=None, min_samples_leaf=8, min_samples_split=2, min_weight_fraction_leaf=0.0, n_estimators=60, n_jobs=-1, oob_score=False, random_state=None, verbose=0, warm_start=False), KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski', metric_params=None, n_jobs=1, n_neighbors=32, p=2, weights='uniform'), SVC(C=1, cache_size=200, class_weight=None, coef0=0.0, decision_function_shape=None, degree=3, gamma=0.25, kernel='rbf', max_iter=-1, probability=False, random_state=None, shrinking=True, tol=0.001, verbose=False), GaussianNB(), SVC(C=0.001, cache_size=200, class_weight=None, gamma='auto', kernel='linear') ]\n",
    "\n",
    "    elif group == 'jovem':\n",
    "        classifiers_agrad = [ ExtraTreesClassifier(bootstrap=False, class_weight=None, criterion='entropy', max_depth=None, max_features='auto',  max_leaf_nodes=None, min_samples_leaf=4, min_samples_split=16, min_weight_fraction_leaf=0.0, n_estimators=20, n_jobs=-1,            oob_score=False, random_state=None, verbose=0, warm_start=False), KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski', metric_params=None, n_jobs=1, n_neighbors=4, p=3,           weights='uniform'), SVC(C=1, cache_size=200, class_weight=None, coef0=0.0, decision_function_shape=None, degree=3, gamma=0.25, kernel='rbf',   max_iter=-1, probability=False, random_state=None, shrinking=True, tol=0.001, verbose=False), GaussianNB(), SVC(C=0.001, cache_size=200, class_weight=None, gamma='auto', kernel='linear') ]\n",
    "        classifiers_seg = [ ExtraTreesClassifier(bootstrap=False, class_weight=None, criterion='entropy', max_depth=None, max_features='auto', max_leaf_nodes=None, min_samples_leaf=8, min_samples_split=4, min_weight_fraction_leaf=0.0, n_estimators=60, n_jobs=-1,            oob_score=False, random_state=None, verbose=0, warm_start=False), KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski', metric_params=None, n_jobs=1, n_neighbors=16, p=3,          weights='uniform'), SVC(C=1, cache_size=200, class_weight=None, coef0=0.0, decision_function_shape=None, degree=3, gamma=0.25, kernel='rbf',   max_iter=-1, probability=False, random_state=None, shrinking=True, tol=0.001, verbose=False), GaussianNB(), SVC(C=0.001, cache_size=200, class_weight=None, gamma='auto', kernel='linear') ]\n",
    "\n",
    "    elif group == 'adulto':\n",
    "        classifiers_agrad = [ ExtraTreesClassifier(bootstrap=False, class_weight=None, criterion='entropy',  max_depth=None, max_features='auto', max_leaf_nodes=None, min_samples_leaf=2, min_samples_split=16, min_weight_fraction_leaf=0.0, n_estimators=20, n_jobs=-1, oob_score=False, random_state=None, verbose=0, warm_start=False), KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski', metric_params=None, n_jobs=1, n_neighbors=32, p=2, weights='uniform'), SVC(C=1, cache_size=200, class_weight=None, coef0=0.0, decision_function_shape=None, degree=3, gamma=0.25, kernel='rbf', max_iter=-1, probability=False, random_state=None, shrinking=True, tol=0.001, verbose=False), GaussianNB(), SVC(C=0.001, cache_size=200, class_weight=None, gamma='auto', kernel='linear') ]\n",
    "        classifiers_seg = [ ExtraTreesClassifier(bootstrap=False, class_weight=None, criterion='entropy', max_depth=None, max_features='auto', max_leaf_nodes=None, min_samples_leaf=32, min_samples_split=16, min_weight_fraction_leaf=0.0, n_estimators=40, n_jobs=-1, oob_score=False, random_state=None, verbose=0, warm_start=False), KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski', metric_params=None, n_jobs=1, n_neighbors=32, p=2, weights='uniform'), SVC(C=1, cache_size=200, class_weight=None, coef0=0.0, decision_function_shape=None, degree=3, gamma=0.25, kernel='rbf', max_iter=-1, probability=False, random_state=None, shrinking=True, tol=0.001, verbose=False), GaussianNB(), SVC(C=0.001, cache_size=200, class_weight=None, gamma='auto', kernel='linear') ]\n",
    "\n",
    "    elif group == 'baixa':\n",
    "        classifiers_agrad = [ ExtraTreesClassifier(bootstrap=False, class_weight=None, criterion='entropy', max_depth=None, max_features='auto', max_leaf_nodes=None, min_samples_leaf=8, min_samples_split=8, min_weight_fraction_leaf=0.0, n_estimators=60, n_jobs=-1, oob_score=False, random_state=None, verbose=0, warm_start=False), KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski', metric_params=None, n_jobs=1, n_neighbors=16, p=2, weights='uniform'), SVC(C=1, cache_size=200, class_weight=None, coef0=0.0, decision_function_shape=None, degree=3, gamma=0.25, kernel='rbf', max_iter=-1, probability=False, random_state=None, shrinking=True, tol=0.001, verbose=False), GaussianNB(), SVC(C=0.001, cache_size=200, class_weight=None, gamma='auto', kernel='linear') ]\n",
    "        classifiers_seg = [ ExtraTreesClassifier(bootstrap=False, class_weight=None, criterion='entropy', max_depth=None, max_features='auto', max_leaf_nodes=None, min_samples_leaf=32, min_samples_split=4, min_weight_fraction_leaf=0.0, n_estimators=20, n_jobs=-1, oob_score=False, random_state=None, verbose=0, warm_start=False), KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski', metric_params=None, n_jobs=1, n_neighbors=32, p=2, weights='uniform'), SVC(C=1, cache_size=200, class_weight=None, coef0=0.0, decision_function_shape=None, degree=3, gamma=0.25, kernel='rbf', max_iter=-1, probability=False, random_state=None, shrinking=True, tol=0.001, verbose=False), GaussianNB(), SVC(C=0.001, cache_size=200, class_weight=None, gamma='auto', kernel='linear') ]\n",
    "\n",
    "    elif group == 'media':\n",
    "        classifiers_agrad = [ ExtraTreesClassifier(bootstrap=False, class_weight=None, criterion='entropy',  max_depth=None, max_features='auto', max_leaf_nodes=None, min_samples_leaf=8, min_samples_split=16, min_weight_fraction_leaf=0.0, n_estimators=20, n_jobs=-1, oob_score=False, random_state=None, verbose=0, warm_start=False), KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski', metric_params=None, n_jobs=1, n_neighbors=8, p=2, weights='uniform'), SVC(C=1, cache_size=200, class_weight=None, coef0=0.0, decision_function_shape=None, degree=3, gamma=0.25, kernel='rbf', max_iter=-1, probability=False, random_state=None, shrinking=True, tol=0.001, verbose=False), GaussianNB(), SVC(C=0.001, cache_size=200, class_weight=None, gamma='auto', kernel='linear') ]\n",
    "        classifiers_seg = [ ExtraTreesClassifier(bootstrap=False, class_weight=None, criterion='entropy', max_depth=None, max_features='auto', max_leaf_nodes=None, min_samples_leaf=4, min_samples_split=8, min_weight_fraction_leaf=0.0, n_estimators=40, n_jobs=-1, oob_score=False, random_state=None, verbose=0, warm_start=False), KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski', metric_params=None, n_jobs=1, n_neighbors=16, p=2, weights='uniform'), SVC(C=1, cache_size=200, class_weight=None, coef0=0.0, decision_function_shape=None, degree=3, gamma=0.25, kernel='rbf', max_iter=-1, probability=False, random_state=None, shrinking=True, tol=0.001, verbose=False), GaussianNB(), SVC(C=0.001, cache_size=200, class_weight=None, gamma='auto', kernel='linear') ]\n",
    "\n",
    "    elif group == 'solteiro':\n",
    "        classifiers_agrad = [ ExtraTreesClassifier(bootstrap=False, class_weight=None, criterion='entropy', max_depth=None, max_features='auto', max_leaf_nodes=None, min_samples_leaf=2, min_samples_split=32, min_weight_fraction_leaf=0.0, n_estimators=60, n_jobs=-1, oob_score=False, random_state=None, verbose=0, warm_start=False), KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski', metric_params=None, n_jobs=1, n_neighbors=8, p=3, weights='uniform'), SVC(C=1, cache_size=200, class_weight=None, coef0=0.0, decision_function_shape=None, degree=3, gamma=0.25, kernel='rbf', max_iter=-1, probability=False, random_state=None, shrinking=True, tol=0.001, verbose=False), GaussianNB(), SVC(C=0.001, cache_size=200, class_weight=None, gamma='auto', kernel='linear') ]\n",
    "        classifiers_seg = [ ExtraTreesClassifier(bootstrap=False, class_weight=None, criterion='entropy', max_depth=None, max_features='auto', max_leaf_nodes=None, min_samples_leaf=8, min_samples_split=4, min_weight_fraction_leaf=0.0, n_estimators=40, n_jobs=-1, oob_score=False, random_state=None, verbose=0, warm_start=False), KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski', metric_params=None, n_jobs=1, n_neighbors=32, p=3, weights='uniform'), SVC(C=1, cache_size=200, class_weight=None, coef0=0.0, decision_function_shape=None, degree=3, gamma=0.25, kernel='rbf', max_iter=-1, probability=False, random_state=None, shrinking=True, tol=0.001, verbose=False), GaussianNB(), SVC(C=0.001, cache_size=200, class_weight=None, gamma='auto', kernel='linear') ]\n",
    "\n",
    "    elif group == 'casado':\n",
    "        classifiers_agrad = [ ExtraTreesClassifier(bootstrap=False, class_weight=None, criterion='entropy', max_depth=None, max_features='auto', max_leaf_nodes=None, min_samples_leaf=4, min_samples_split=2, min_weight_fraction_leaf=0.0, n_estimators=20, n_jobs=-1, oob_score=False, random_state=None, verbose=0, warm_start=False), KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski', metric_params=None, n_jobs=1, n_neighbors=32, p=2, weights='uniform'), SVC(C=1, cache_size=200, class_weight=None, coef0=0.0, decision_function_shape=None, degree=3, gamma=0.25, kernel='rbf', max_iter=-1, probability=False, random_state=None, shrinking=True, tol=0.001, verbose=False), GaussianNB(), SVC(C=0.001, cache_size=200, class_weight=None, gamma='auto', kernel='linear') ]\n",
    "        classifiers_seg = [ ExtraTreesClassifier(bootstrap=False, class_weight=None, criterion='entropy', max_depth=None, max_features='auto', max_leaf_nodes=None, min_samples_leaf=8, min_samples_split=2, min_weight_fraction_leaf=0.0, n_estimators=40, n_jobs=-1, oob_score=False, random_state=None, verbose=0, warm_start=False), KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',  metric_params=None, n_jobs=1, n_neighbors=16, p=3, weights='uniform'), SVC(C=1, cache_size=200, class_weight=None, coef0=0.0, decision_function_shape=None, degree=3, gamma=0.25, kernel='rbf', max_iter=-1, probability=False, random_state=None, shrinking=True, tol=0.001, verbose=False), GaussianNB(), SVC(C=0.001, cache_size=200, class_weight=None, gamma='auto', kernel='linear') ]\n",
    "\n",
    "    elif group == \"modal\":#All users (more common param for each group)\n",
    "        classifiers_agrad = [ ExtraTreesClassifier(class_weight=None, criterion='entropy', min_samples_leaf=8, min_samples_split=16, n_estimators=20, n_jobs=-1, oob_score=False) ]\n",
    "\n",
    "        classifiers_seg = [ ExtraTreesClassifier(class_weight=None, criterion='entropy', min_samples_leaf=8, min_samples_split=4, n_estimators=40, n_jobs=-1, oob_score=False) ]\n",
    "\n",
    "    elif group == \"all\":#80-20 best configuration for classifiers with all features in!\n",
    "        classifiers_agrad = [ ExtraTreesClassifier(bootstrap=False, class_weight=None, criterion='entropy',           max_depth=None, max_features='auto', max_leaf_nodes=None, min_samples_leaf=8, min_samples_split=32,           min_weight_fraction_leaf=0.0, n_estimators=60, n_jobs=-1, oob_score=False, random_state=None, verbose=0, warm_start=False) ]\n",
    "\n",
    "        classifiers_seg = [ ExtraTreesClassifier(bootstrap=False, class_weight=None, criterion='entropy',           max_depth=None, max_features='auto', max_leaf_nodes=None, min_samples_leaf=16, min_samples_split=2,            min_weight_fraction_leaf=0.0, n_estimators=60, n_jobs=-1, oob_score=False, random_state=None, verbose=0, warm_start=False) ]\n",
    "\n",
    "    else:\n",
    "        classifiers_agrad = [ ExtraTreesClassifier(bootstrap=False, class_weight=None, criterion='entropy',           max_depth=None, max_features='auto', max_leaf_nodes=None, min_samples_leaf=16, min_samples_split=32,           min_weight_fraction_leaf=0.0, n_estimators=60, n_jobs=-1, oob_score=False, random_state=None, verbose=0, warm_start=False) ]\n",
    "\n",
    "        classifiers_seg = [ ExtraTreesClassifier(bootstrap=False, class_weight=None, criterion='entropy',           max_depth=None, max_features='auto', max_leaf_nodes=None, min_samples_leaf=8, min_samples_split=8,           min_weight_fraction_leaf=0.0, n_estimators=60, n_jobs=-1, oob_score=False, random_state=None, verbose=0, warm_start=False) ]\n",
    "\n",
    "    return [classifiers_agrad, classifiers_seg]\n",
    "\n",
    "def load_classifiers_wodraw(group):\n",
    "    #Building classifiers according to best configuration per group (SO EXTRA ATUALIZADO!)\n",
    "    if group == 'masculino':\n",
    "        classifiers_agrad = [ ExtraTreesClassifier(bootstrap=False, class_weight=None, criterion='entropy', max_depth=None, max_features='auto', max_leaf_nodes=None, min_samples_leaf=8, min_samples_split=2, min_weight_fraction_leaf=0.0, n_estimators=60, n_jobs=-1, oob_score=False, random_state=None, verbose=0, warm_start=False), KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski', metric_params=None, n_jobs=1, n_neighbors=4, p=2, weights='uniform'), SVC(C=1, cache_size=200, class_weight='balanced', coef0=0.0, decision_function_shape=None, degree=3, gamma=0.25, kernel='rbf', max_iter=-1, probability=False, random_state=None, shrinking=True,  tol=0.001, verbose=False), GaussianNB(), SVC(C=0.001, cache_size=200, class_weight=None, gamma='auto', kernel='linear') ]\n",
    "        classifiers_seg = [ ExtraTreesClassifier(bootstrap=False, class_weight=None, criterion='entropy', max_depth=None, max_features='auto', max_leaf_nodes=None, min_samples_leaf=4, min_samples_split=8,           min_weight_fraction_leaf=0.0, n_estimators=40, n_jobs=-1, oob_score=False, random_state=None, verbose=0, warm_start=False), KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski', metric_params=None, n_jobs=1, n_neighbors=16, p=2, weights='uniform'), SVC(C=1, cache_size=200, class_weight=None, coef0=0.0, decision_function_shape=None, degree=3, gamma=0.5, kernel='rbf', max_iter=-1, probability=False, random_state=None, shrinking=True, tol=0.001, verbose=False), GaussianNB(), SVC(C=0.001, cache_size=200, class_weight=None, gamma='auto', kernel='linear') ]\n",
    "\n",
    "    elif group == 'feminino':\n",
    "        classifiers_agrad = [ ExtraTreesClassifier(bootstrap=False, class_weight=None, criterion='entropy', max_depth=None, max_features='auto', max_leaf_nodes=None,  min_samples_leaf=2, min_samples_split=4,           min_weight_fraction_leaf=0.0, n_estimators=40, n_jobs=-1, oob_score=False, random_state=None, verbose=0, warm_start=False), KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski', metric_params=None, n_jobs=1, n_neighbors=32, p=2, weights='uniform'), SVC(C=1, cache_size=200, class_weight='balanced', coef0=0.0, decision_function_shape=None, degree=3, gamma=0.25, kernel='rbf', max_iter=-1, probability=False, random_state=None, shrinking=True, tol=0.001, verbose=False), GaussianNB(), SVC(C=0.001, cache_size=200, class_weight=None, gamma='auto', kernel='linear') ]\n",
    "        classifiers_seg = [ ExtraTreesClassifier(bootstrap=False, class_weight=None, criterion='entropy', max_depth=None, max_features='auto', max_leaf_nodes=None, min_samples_leaf=4, min_samples_split=8,            min_weight_fraction_leaf=0.0, n_estimators=20, n_jobs=-1, oob_score=False, random_state=None, verbose=0, warm_start=False), KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski', metric_params=None, n_jobs=1, n_neighbors=32, p=2, weights='uniform'), SVC(C=1, cache_size=200, class_weight=None, coef0=0.0, decision_function_shape=None, degree=3, gamma=0.25, kernel='rbf', max_iter=-1, probability=False, random_state=None, shrinking=True, tol=0.001, verbose=False), GaussianNB(), SVC(C=0.001, cache_size=200, class_weight=None, gamma='auto', kernel='linear') ]\n",
    "\n",
    "    elif group == 'jovem':\n",
    "        classifiers_agrad = [ ExtraTreesClassifier(bootstrap=False, class_weight=None, criterion='entropy', max_depth=None, max_features='auto', max_leaf_nodes=None, min_samples_leaf=2, min_samples_split=32, min_weight_fraction_leaf=0.0, n_estimators=60, n_jobs=-1,           oob_score=False, random_state=None, verbose=0, warm_start=False), KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski', metric_params=None, n_jobs=1, n_neighbors=32, p=2,           weights='uniform'), SVC(C=1, cache_size=200, class_weight='balanced', coef0=0.0, decision_function_shape=None, degree=3, gamma=0.25, kernel='rbf',   max_iter=-1, probability=False, random_state=None, shrinking=True, tol=0.001, verbose=False), GaussianNB(), SVC(C=0.1, cache_size=200, class_weight=None, coef0=0.0, decision_function_shape=None, degree=3, gamma='auto', kernel='linear',   max_iter=-1, probability=False, random_state=None, shrinking=True, tol=0.001, verbose=False) ]\n",
    "        classifiers_seg = [ ExtraTreesClassifier(bootstrap=False, class_weight=None, criterion='entropy', max_depth=None, max_features='auto', max_leaf_nodes=None, min_samples_leaf=2, min_samples_split=16, min_weight_fraction_leaf=0.0, n_estimators=40, n_jobs=-1,            oob_score=False, random_state=None, verbose=0, warm_start=False), KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski', metric_params=None, n_jobs=1, n_neighbors=16, p=2,           weights='uniform'), SVC(C=1, cache_size=200, class_weight='balanced', coef0=0.0, decision_function_shape=None, degree=3, gamma=0.25, kernel='rbf',   max_iter=-1, probability=False, random_state=None, shrinking=True, tol=0.001, verbose=False), GaussianNB(), SVC(C=0.001, cache_size=200, class_weight=None, coef0=0.0, decision_function_shape=None, degree=3, gamma='auto', kernel='linear',   max_iter=-1, probability=False, random_state=None, shrinking=True, tol=0.001, verbose=False) ]\n",
    "\n",
    "    elif group == 'adulto':\n",
    "        classifiers_agrad = [ ExtraTreesClassifier(bootstrap=False, class_weight=None, criterion='entropy', max_depth=None, max_features='auto', max_leaf_nodes=None, min_samples_leaf=8, min_samples_split=16,            min_weight_fraction_leaf=0.0, n_estimators=20, n_jobs=-1, oob_score=False, random_state=None, verbose=0, warm_start=False), KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski', metric_params=None, n_jobs=1, n_neighbors=16, p=2, weights='uniform'), SVC(C=1, cache_size=200, class_weight=None, coef0=0.0, decision_function_shape=None, degree=3, gamma=0.25, kernel='rbf', max_iter=-1, probability=False, random_state=None, shrinking=True, tol=0.001, verbose=False), GaussianNB(), SVC(C=0.001, cache_size=200, class_weight=None, gamma='auto', kernel='linear') ]\n",
    "        classifiers_seg = [ ExtraTreesClassifier(bootstrap=False, class_weight=None, criterion='entropy', max_depth=None, max_features='auto', max_leaf_nodes=None, min_samples_leaf=8, min_samples_split=4,            min_weight_fraction_leaf=0.0, n_estimators=60, n_jobs=-1, oob_score=False, random_state=None, verbose=0, warm_start=False), KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski', metric_params=None, n_jobs=1, n_neighbors=32, p=2, weights='uniform'), SVC(C=1, cache_size=200, class_weight=None, coef0=0.0, decision_function_shape=None, degree=3, gamma=0.25, kernel='rbf', max_iter=-1, probability=False, random_state=None, shrinking=True, tol=0.001, verbose=False), GaussianNB(), SVC(C=0.001, cache_size=200, class_weight=None, gamma='auto', kernel='linear') ]\n",
    "\n",
    "    elif group == 'baixa':\n",
    "        classifiers_agrad = [ ExtraTreesClassifier(bootstrap=False, class_weight=None, criterion='entropy', max_depth=None, max_features='auto', max_leaf_nodes=None, min_samples_leaf=2, min_samples_split=32,            min_weight_fraction_leaf=0.0, n_estimators=60, n_jobs=-1, oob_score=False, random_state=None, verbose=0, warm_start=False), KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',  metric_params=None, n_jobs=1, n_neighbors=32, p=2, weights='uniform'), SVC(C=1, cache_size=200, class_weight=None, coef0=0.0, decision_function_shape=None, degree=3, gamma=0.25, kernel='rbf', max_iter=-1, probability=False, random_state=None, shrinking=True, tol=0.001, verbose=False), GaussianNB(), SVC(C=0.001, cache_size=200, class_weight=None, gamma='auto', kernel='linear') ]\n",
    "        classifiers_seg = [ ExtraTreesClassifier(bootstrap=False, class_weight=None, criterion='entropy', max_depth=None, max_features='auto', max_leaf_nodes=None, min_samples_leaf=4, min_samples_split=2,            min_weight_fraction_leaf=0.0, n_estimators=20, n_jobs=-1, oob_score=False, random_state=None, verbose=0, warm_start=False), KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski', metric_params=None, n_jobs=1, n_neighbors=16, p=3, weights='uniform'), SVC(C=1, cache_size=200, class_weight='balanced', coef0=0.0, decision_function_shape=None, degree=3, gamma=0.25, kernel='rbf', max_iter=-1, probability=False, random_state=None, shrinking=True, tol=0.001, verbose=False), GaussianNB(), SVC(C=0.001, cache_size=200, class_weight=None, gamma='auto', kernel='linear') ]\n",
    "\n",
    "    elif group == 'media':\n",
    "        classifiers_agrad = [ ExtraTreesClassifier(bootstrap=False, class_weight=None, criterion='entropy', max_depth=None, max_features='auto', max_leaf_nodes=None, min_samples_leaf=8, min_samples_split=2,           min_weight_fraction_leaf=0.0, n_estimators=60, n_jobs=-1, oob_score=False, random_state=None, verbose=0, warm_start=False), KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski', metric_params=None, n_jobs=1, n_neighbors=16, p=2, weights='uniform'), SVC(C=1, cache_size=200, class_weight='balanced', coef0=0.0, decision_function_shape=None, degree=3, gamma=0.25, kernel='rbf', max_iter=-1, probability=False, random_state=None, shrinking=True, tol=0.001, verbose=False), GaussianNB(), SVC(C=0.001, cache_size=200, class_weight=None, gamma='auto', kernel='linear') ]\n",
    "        classifiers_seg = [ ExtraTreesClassifier(bootstrap=False, class_weight=None, criterion='entropy', max_depth=None, max_features='auto', max_leaf_nodes=None, min_samples_leaf=4, min_samples_split=4,            min_weight_fraction_leaf=0.0, n_estimators=40, n_jobs=-1, oob_score=False, random_state=None, verbose=0, warm_start=False), KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski', metric_params=None, n_jobs=1, n_neighbors=8, p=3, weights='uniform'), SVC(C=1, cache_size=200, class_weight=None, coef0=0.0, decision_function_shape=None, degree=3, gamma=0.25, kernel='rbf', max_iter=-1, probability=False, random_state=None, shrinking=True, tol=0.001, verbose=False), GaussianNB(), SVC(C=0.001, cache_size=200, class_weight=None, gamma='auto', kernel='linear') ]\n",
    "\n",
    "    elif group == 'solteiro':\n",
    "        classifiers_agrad = [ ExtraTreesClassifier(bootstrap=False, class_weight=None, criterion='entropy', max_depth=None, max_features='auto', max_leaf_nodes=None, min_samples_leaf=2, min_samples_split=8,            min_weight_fraction_leaf=0.0, n_estimators=60, n_jobs=-1, oob_score=False, random_state=None, verbose=0, warm_start=False), KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski', metric_params=None, n_jobs=1, n_neighbors=8, p=2, weights='uniform'), SVC(C=1, cache_size=200, class_weight=None, coef0=0.0, decision_function_shape=None, degree=3, gamma=0.25, kernel='rbf', max_iter=-1, probability=False, random_state=None, shrinking=True, tol=0.001, verbose=False), GaussianNB(), SVC(C=0.001, cache_size=200, class_weight=None, gamma='auto', kernel='linear') ]\n",
    "        classifiers_seg = [ ExtraTreesClassifier(bootstrap=False, class_weight=None, criterion='entropy', max_depth=None, max_features='auto', max_leaf_nodes=None, min_samples_leaf=4, min_samples_split=2,            min_weight_fraction_leaf=0.0, n_estimators=40, n_jobs=-1, oob_score=False, random_state=None, verbose=0, warm_start=False), KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski', metric_params=None, n_jobs=1, n_neighbors=32, p=2, weights='uniform'), SVC(C=1, cache_size=200, class_weight=None, coef0=0.0, decision_function_shape=None, degree=3, gamma=0.25, kernel='rbf', max_iter=-1, probability=False, random_state=None, shrinking=True, tol=0.001, verbose=False), GaussianNB(), SVC(C=0.001, cache_size=200, class_weight=None, gamma='auto', kernel='linear') ]\n",
    "\n",
    "    elif group == 'casado':\n",
    "        classifiers_agrad = [ ExtraTreesClassifier(bootstrap=False, class_weight=None, criterion='entropy', max_depth=None, max_features='auto', max_leaf_nodes=None, min_samples_leaf=8, min_samples_split=4,            min_weight_fraction_leaf=0.0, n_estimators=20, n_jobs=-1, oob_score=False, random_state=None, verbose=0, warm_start=False), KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski', metric_params=None, n_jobs=1, n_neighbors=16, p=2, weights='uniform'), SVC(C=1, cache_size=200, class_weight='balanced', coef0=0.0, decision_function_shape=None, degree=3, gamma=0.25, kernel='rbf', max_iter=-1, probability=False, random_state=None, shrinking=True, tol=0.001, verbose=False), GaussianNB(), SVC(C=0.001, cache_size=200, class_weight=None, gamma='auto', kernel='linear') ]\n",
    "        classifiers_seg = [ ExtraTreesClassifier(bootstrap=False, class_weight=None, criterion='entropy', max_depth=None, max_features='auto', max_leaf_nodes=None, min_samples_leaf=2, min_samples_split=16,            min_weight_fraction_leaf=0.0, n_estimators=60, n_jobs=-1, oob_score=False, random_state=None, verbose=0, warm_start=False), KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski', metric_params=None, n_jobs=1, n_neighbors=32, p=3, weights='uniform'), SVC(C=1, cache_size=200, class_weight=None, coef0=0.0, decision_function_shape=None, degree=3, gamma=0.25, kernel='rbf', max_iter=-1, probability=False, random_state=None, shrinking=True, tol=0.001, verbose=False), GaussianNB(), SVC(C=0.001, cache_size=200, class_weight=None, gamma='auto', kernel='linear') ]\n",
    "\n",
    "    elif group == \"modal\":#All users (more common param for each group - Extra)\n",
    "        classifiers_agrad = [ ExtraTreesClassifier(class_weight=None, criterion='entropy', min_samples_leaf=8, min_samples_split=2, n_estimators=60, n_jobs=-1, oob_score=False) ]\n",
    "\n",
    "        classifiers_seg = [ ExtraTreesClassifier(class_weight=None, criterion='entropy', min_samples_leaf=4, min_samples_split=8, n_estimators=40, n_jobs=-1, oob_score=False) ]\n",
    "\n",
    "    elif group == \"all\":#All features in best configuration 80-20\n",
    "        classifiers_agrad = [ ExtraTreesClassifier(bootstrap=False, class_weight=None, criterion='entropy',           max_depth=None, max_features='auto', max_leaf_nodes=None, min_samples_leaf=2, min_samples_split=32,         min_weight_fraction_leaf=0.0, n_estimators=60, n_jobs=-1, oob_score=False, random_state=None, verbose=0, warm_start=False) ]\n",
    "\n",
    "        classifiers_seg = [ ExtraTreesClassifier(bootstrap=False, class_weight=None, criterion='entropy',           max_depth=None, max_features='auto', max_leaf_nodes=None, min_samples_leaf=2, min_samples_split=4,           min_weight_fraction_leaf=0.0, n_estimators=60, n_jobs=-1, oob_score=False, random_state=None, verbose=0, warm_start=False) ]\n",
    "\n",
    "    else:\n",
    "        classifiers_agrad = [ ExtraTreesClassifier(bootstrap=False, class_weight=None, criterion='entropy',           max_depth=None, max_features='auto', max_leaf_nodes=None, min_samples_leaf=8, min_samples_split=8,          min_weight_fraction_leaf=0.0, n_estimators=60, n_jobs=-1, oob_score=False, random_state=None, verbose=0, warm_start=False) ]\n",
    "\n",
    "        classifiers_seg = [ ExtraTreesClassifier(bootstrap=False, class_weight=None, criterion='entropy',           max_depth=None, max_features='auto', max_leaf_nodes=None, min_samples_leaf=8, min_samples_split=8,            min_weight_fraction_leaf=0.0, n_estimators=60, n_jobs=-1, oob_score=False, random_state=None, verbose=0, warm_start=False) ]\n",
    "\n",
    "\n",
    "    return [classifiers_agrad, classifiers_seg]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def convertColumnsToDummy(df):\n",
    "    \"\"\" Converts categorical features to dummy variables in the data frame \"\"\"\n",
    "\n",
    "    #Users categorical information to dummy!\t\n",
    "    res = pd.get_dummies(df['gender'])\n",
    "    df = df.join(res)\n",
    "    res = pd.get_dummies(df['income'])\n",
    "    df = df.join(res)\n",
    "    res = pd.get_dummies(df['marital'])\n",
    "    df = df.join(res)\n",
    "    res = pd.get_dummies(df['education'])\n",
    "    df = df.join(res)\n",
    "\n",
    "    #Images categorical information to dummy!\n",
    "    res = pd.get_dummies(df['bairro1'], prefix=\"bairro1\")\n",
    "    df = df.join(res)\n",
    "    res = pd.get_dummies(df['graffiti1'], prefix=\"graffiti1\")\n",
    "    df = df.join(res)\n",
    "    res = pd.get_dummies(df['bairro2'], prefix=\"bairro2\")\n",
    "    df = df.join(res)\n",
    "    res = pd.get_dummies(df['graffiti2'], prefix=\"graffiti2\")\n",
    "    df = df.join(res)\n",
    "\n",
    "    return df\n",
    "\n",
    "def stripDataFrame(df):\n",
    "    \"\"\" Removes unused chars from dataframes columns values \"\"\"\n",
    "\n",
    "    df['gender'] = [x.lstrip(' \\t\\n\\r').rstrip(' \\t\\n\\r') for x in df['gender']]\n",
    "    df['marital'] = [x.lstrip(' \\t\\n\\r').rstrip(' \\t\\n\\r') for x in df['marital']]\n",
    "    df['income'] = [x.lstrip(' \\t\\n\\r').rstrip(' \\t\\n\\r') for x in df['income']]\n",
    "    df['graffiti1'] = [x.lstrip(' \\t\\n\\r').rstrip(' \\t\\n\\r') for x in df['graffiti1']]\n",
    "    df['graffiti2'] = [x.lstrip(' \\t\\n\\r').rstrip(' \\t\\n\\r') for x in df['graffiti2']]\n",
    "    df['bairro1'] = [x.lstrip(' \\t\\n\\r').rstrip(' \\t\\n\\r') for x in df['bairro1']]\n",
    "    df['bairro2'] = [x.lstrip(' \\t\\n\\r').rstrip(' \\t\\n\\r') for x in df['bairro2']]\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def explainClassification(headers, target_names, X_train_scaled, X_test_scaled, clf, index):\n",
    "    #explainClassification(list_of_predictors, current_df['choice'].unique(), predictors_test, clf, index)\n",
    "\n",
    "    #c = make_pipeline(vectorizer, clf)\n",
    "    #headers = np.array(['age', 'gender', 'income', 'educ', 'marital', 'street_wid1', 'mov_cars1', 'park_cars1', 'mov_ciclyst1', 'landscape1', 'build_ident1', 'trees1', 'build_height1', 'diff_build1', 'people1', 'graffiti1', 'bairro1', 'street_wid2', 'mov_cars2', 'park_cars2', 'mov_ciclyst2', 'landscape2', 'build_ident2', 'trees2', 'build_height2', 'diff_build2', 'people2', 'graffiti2', 'bairro2'])\n",
    "    target_names = np.array(map(str, target_names))\n",
    "    #TODO ERROR HERE! PRINT ALL\n",
    "    \n",
    "    #print(\"HEADERS \" + str(headers) + \" \" + str(len(headers)))\n",
    "    #print(\"TARGET \" + str(target_names)+ \" \" + str(len(target_names)))\n",
    "    #print(\"predi \" + str(predictors[1:5]) + \" \" + str(len(predictors)))\n",
    "    #print(\"answer \" + str(answer)+ \" \" + str(len(answer)))\n",
    "    #print(\"CLF \" + str(clf))\n",
    "    #print(\"INDEX \" + str(answer[index]))\n",
    "    #print(\"PROBA \" + str(clf.predict_proba)+ \" \" + str(clf.predict_proba))\n",
    "\n",
    "    explainer = LimeTabularExplainer(X_train_scaled, feature_names=headers, class_names=target_names, \n",
    "                                     discretize_continuous=True)\n",
    "    exp = explainer.explain_instance(X_test_scaled[index], clf.predict_proba, num_features=len(headers), \n",
    "                                     top_labels=1)\n",
    "    #print (str(headers))\n",
    "    #print (str(clf.feature_importances_))\n",
    "    #print (str(exp.as_map()))\n",
    "    \n",
    "    #exp.show_in_notebook(show_table=True, show_all=False)\n",
    "    return exp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Question\tPleasantness\t\n",
      "User\t617\n",
      "User\t814\n",
      "0\tage\t0.000329436185569\t0.0029582232768\n",
      "1\tmasculino\t0.0\t0.0\n",
      "2\tfeminino\t-0.00262132923661\t0.00665573650085\n",
      "3\tbaixa\t0.00359218165723\t0.00906157142301\n",
      "4\tmedia baixa\t-0.00511457398641\t0.0139513158023\n",
      "5\tmedia\t-0.000874151993304\t0.00323906742161\n",
      "6\tmedia alta\t0.0002012168444\t0.0037249943737\n",
      "7\tsolteiro\t0.0\t0.0\n",
      "8\tcasado\t0.000249921023644\t0.00377991431337\n",
      "9\tstreet_wid1\t0.0105308705983\t0.0254277840659\n",
      "10\tmov_cars1\t-0.000522658334143\t0.0111473948213\n",
      "11\tpark_cars1\t-0.00312999218298\t0.0166281294776\n",
      "12\tmov_ciclyst1\t0.00282828550725\t0.00784440037506\n",
      "13\tlandscape1\t0.0499373226954\t0.0786763206446\n",
      "14\tbuild_ident1\t-0.000350033834782\t0.00947255813015\n",
      "15\ttrees1\t0.00921132687651\t0.0264948145002\n",
      "16\tbuild_height1\t0.000145922305755\t0.00518725728983\n",
      "17\tdiff_build1\t-0.00080314588023\t0.00623418970284\n",
      "18\tpeople1\t-0.000256428904022\t0.00315205583302\n",
      "19\tgraffiti1_No\t0.0\t0.0\n",
      "20\tgraffiti1_Yes\t0.00257049358835\t0.00652858717992\n",
      "21\tbairro1_catole\t0.00585310421033\t0.0397713048218\n",
      "22\tbairro1_centro\t0.00233479524644\t0.00442744309179\n",
      "23\tbairro1_liberdade\t0.00258622494796\t0.0377524834249\n",
      "24\tstreet_wid2\t0.00334009344281\t0.0102680911133\n",
      "25\tmov_cars2\t9.93231261106e-05\t0.00329740697065\n",
      "26\tpark_cars2\t0.000728751735858\t0.00304475409643\n",
      "27\tmov_ciclyst2\t0.000355071277132\t0.00566194233712\n",
      "28\tlandscape2\t0.0156270890757\t0.0328175278141\n",
      "29\tbuild_ident2\t0.00087454633613\t0.00396858396801\n",
      "30\ttrees2\t0.00729607501785\t0.0151635663153\n",
      "31\tbuild_height2\t-0.000294359855209\t0.00353547913865\n",
      "32\tdiff_build2\t-0.000163760631728\t0.00322983673847\n",
      "33\tpeople2\t0.00247980728514\t0.00518899934761\n",
      "34\tgraffiti2_No\t0.0\t0.0\n",
      "35\tgraffiti2_Yes\t-0.000758697074377\t0.00649967010738\n",
      "36\tbairro2_catole\t0.0100398409099\t0.0256187283021\n",
      "37\tbairro2_centro\t0.00805678443953\t0.0223072071731\n",
      "38\tbairro2_liberdade\t0.00110096994442\t0.00319821802726\n",
      ">>> Question\tSafety\t\n",
      "User\t617\n",
      "User\t654\n",
      "0\tage\t0.00060931204288\t0.00342892747048\n",
      "1\tmasculino\t0.00107882449335\t0.0165372707629\n",
      "2\tfeminino\t0.0012667252932\t0.0211902279877\n",
      "3\tbaixa\t0.00136018248704\t0.0114467916223\n",
      "4\tmedia baixa\t0.00201599561985\t0.0152330373846\n",
      "5\tmedia\t0.00252938903185\t0.0246145836858\n",
      "6\tmedia alta\t0.00233486955236\t0.0182666781776\n",
      "7\tsolteiro\t0.0\t0.0\n",
      "8\tcasado\t0.00045981582936\t0.00345843293203\n",
      "9\tstreet_wid1\t0.00800363645455\t0.0254705675928\n",
      "10\tmov_cars1\t0.0104088653084\t0.0378561636544\n",
      "11\tpark_cars1\t0.00307553805722\t0.0153040690561\n",
      "12\tmov_ciclyst1\t0.00105324669447\t0.0110781908323\n",
      "13\tlandscape1\t0.0116040637737\t0.0415235265513\n",
      "14\tbuild_ident1\t0.00349261293543\t0.0205857577736\n",
      "15\ttrees1\t0.000290696385746\t0.00380001534288\n",
      "16\tbuild_height1\t0.00219686042121\t0.0130094624089\n",
      "17\tdiff_build1\t0.00197777391893\t0.0144294776286\n",
      "18\tpeople1\t0.000762075817514\t0.0165987300012\n",
      "19\tgraffiti1_No\t0.0\t0.0\n",
      "20\tgraffiti1_Yes\t-0.00249642318944\t0.0204706788318\n",
      "21\tbairro1_catole\t0.000947090094191\t0.00850615523305\n",
      "22\tbairro1_centro\t0.00199774896054\t0.0229733062147\n",
      "23\tbairro1_liberdade\t0.0025146976146\t0.0384297482938\n",
      "24\tstreet_wid2\t0.00916485980969\t0.0171313307073\n",
      "25\tmov_cars2\t0.0106336458503\t0.0254812969869\n",
      "26\tpark_cars2\t0.000177371479111\t0.0170343265392\n",
      "27\tmov_ciclyst2\t-0.00156830703622\t0.00758113787185\n",
      "28\tlandscape2\t0.0302623013968\t0.0602823826418\n",
      "29\tbuild_ident2\t0.00161330056334\t0.0111417965915\n",
      "30\ttrees2\t0.0138742222202\t0.0217172726897\n",
      "31\tbuild_height2\t0.00133699808945\t0.00575166266392\n",
      "32\tdiff_build2\t-0.000220523607033\t0.00746683324935\n",
      "33\tpeople2\t-0.000819953155785\t0.00703812465378\n",
      "34\tgraffiti2_No\t0.0\t0.0\n",
      "35\tgraffiti2_Yes\t0.0143385070714\t0.0693669216726\n",
      "36\tbairro2_catole\t6.16038889015e-05\t0.0156352615653\n",
      "37\tbairro2_centro\t-0.00116881866399\t0.0224075841912\n",
      "38\tbairro2_liberdade\t-8.30956177764e-05\t0.00570716014212\n"
     ]
    }
   ],
   "source": [
    "#Main!\n",
    "input_file = 'classifier_input_wodraw.dat'\n",
    "\n",
    "#Using 3 classes or two classes as output\n",
    "if \"3classes\" in input_file.lower():\n",
    "    load_3classes = True\n",
    "else:\n",
    "    load_3classes = False\n",
    "\n",
    "df = pd.read_table(input_file, sep='\\t', encoding='utf8', header=0)\n",
    "#Remove unecessary chars!\n",
    "df = stripDataFrame(df)\n",
    "\n",
    "#, (\"gender-masculino\", \"masculino\"), (\"gender-feminino\", \"feminino\"), (\"age-jovem\", \"jovem\"), (\"age-adulto\", \"adulto\"), (\"income-baixa\", \"baixa\"), (\"income-media\", \"media\"), (\"marital-solteiro\", \"solteiro\"), (\"marital-casado\", \"casado\")\n",
    "for groups_data in [ (\"\", \"\")]:\n",
    "\n",
    "    filter_group = groups_data[0]\n",
    "    group = groups_data[1]\n",
    "\n",
    "    if len(filter_group) > 0:\n",
    "        if 'gender' in filter_group:\n",
    "            df_to_use = df[(df.gender == group)]\n",
    "        elif 'marital' in filter_group:\n",
    "            df_to_use = df[(df.marital == group)]\n",
    "        elif 'income' in filter_group:\n",
    "            if group == 'media':\n",
    "                df_to_use = df[(df.income == \"media\") | (df.income == \"media alta\")]\n",
    "            elif group == 'baixa':\n",
    "                df_to_use = df[(df.income == \"baixa\") | (df.income == \"media baixa\")]\n",
    "        elif 'age' in filter_group:\n",
    "            if group == 'adulto':\n",
    "                df_to_use = df[(df.age >= 25)]\n",
    "            elif group == 'jovem':\n",
    "                df_to_use = df[(df.age <= 24)]\n",
    "    else:\n",
    "        df_to_use = df\n",
    "\n",
    "    #Pleasantness and safety data\n",
    "    agrad_df = df_to_use[(df_to_use.question != \"seguro?\")]\n",
    "    agrad_df = convertColumnsToDummy(agrad_df)\n",
    "    list_of_predictors_agrad = ['age', 'masculino', 'feminino', 'baixa', 'media baixa', 'media', 'media alta', \n",
    "                      'solteiro', 'casado', 'street_wid1', 'mov_cars1', 'park_cars1', 'mov_ciclyst1', \n",
    "                      'landscape1', 'build_ident1', 'trees1', 'build_height1', 'diff_build1', 'people1', \n",
    "                      'graffiti1_No', 'graffiti1_Yes', 'bairro1_catole', 'bairro1_centro', 'bairro1_liberdade', \n",
    "                      'street_wid2', 'mov_cars2', 'park_cars2', 'mov_ciclyst2', 'landscape2', 'build_ident2', \n",
    "                      'trees2', 'build_height2', 'diff_build2', 'people2', 'graffiti2_No', 'graffiti2_Yes', \n",
    "                      'bairro2_catole', 'bairro2_centro', 'bairro2_liberdade']\n",
    "    for column in ['masculino', 'feminino', 'baixa', 'media baixa', 'media', 'media alta', 'solteiro', 'casado']:\n",
    "        if not column in agrad_df.columns:\n",
    "            list_of_predictors_agrad.remove(column)\n",
    "    answer_agrad = agrad_df['choice']#Preferred images\n",
    "    predictors_agrad = agrad_df[list_of_predictors_agrad].values #Predictors\n",
    "\n",
    "    seg_df = df_to_use[(df_to_use.question == \"seguro?\")]\n",
    "    seg_df = convertColumnsToDummy(seg_df)\n",
    "    list_of_predictors_seg = ['age', 'masculino', 'feminino', 'baixa', 'media baixa', 'media', 'media alta', \n",
    "                      'solteiro', 'casado', 'street_wid1', 'mov_cars1', 'park_cars1', 'mov_ciclyst1', \n",
    "                      'landscape1', 'build_ident1', 'trees1', 'build_height1', 'diff_build1', 'people1', \n",
    "                      'graffiti1_No', 'graffiti1_Yes', 'bairro1_catole', 'bairro1_centro', 'bairro1_liberdade', \n",
    "                      'street_wid2', 'mov_cars2', 'park_cars2', 'mov_ciclyst2', 'landscape2', 'build_ident2', \n",
    "                      'trees2', 'build_height2', 'diff_build2', 'people2', 'graffiti2_No', 'graffiti2_Yes', \n",
    "                      'bairro2_catole', 'bairro2_centro', 'bairro2_liberdade']\n",
    "    for column in ['masculino', 'feminino', 'baixa', 'media baixa', 'media', 'media alta', 'solteiro', 'casado']:\n",
    "        if not column in seg_df.columns:\n",
    "            list_of_predictors_seg.remove(column)\n",
    "    answer_seg = seg_df['choice']#Preferred images\n",
    "    predictors_seg = seg_df[list_of_predictors_seg].values #Predictors\n",
    "\n",
    "    #Loading classifiers\n",
    "    if load_3classes:\n",
    "        classifiers = load_classifiers_3classes(\"modal\")\n",
    "    else:\n",
    "        classifiers = load_classifiers_wodraw(\"modal\")\n",
    "    classifiers_agrad = classifiers[0]#[RandomForestClassifier(n_estimators=120, max_depth=5, min_samples_split=2, \n",
    "                                                #min_samples_leaf=2)]\n",
    "    classifiers_seg = classifiers[1]#[RandomForestClassifier(n_estimators=120, max_depth=5, min_samples_split=2, \n",
    "                                              #min_samples_leaf=2)]\n",
    "\n",
    "    #Evaluate each data frame\n",
    "    data_frames = [agrad_df, seg_df]\n",
    "    for index_df in range(0, len(data_frames)):\n",
    "        current_df = data_frames[index_df]\n",
    "        current_df = current_df.sort_values(by='choice', ascending=True)\n",
    "        user_ids = current_df['userID'].unique()#Selecting users\n",
    "        relevance_map = {}\n",
    "        probabilities_map = {}\n",
    "\n",
    "        if index_df == 0:#Selecting predictors\n",
    "            list_of_predictors = list_of_predictors_agrad\n",
    "        else:\n",
    "            list_of_predictors = list_of_predictors_seg\n",
    "\n",
    "        print( \">>> Question\\t\" + str((\"Safety\", \"Pleasantness\")[index_df == 0]) + \"\\t\" + group )\n",
    "\n",
    "        for user_id in user_ids[0:2]:#Remove each user sequentially\n",
    "            print(\"User\\t\" + str(user_id))\n",
    "\n",
    "            current_df_train = current_df[(current_df.userID != user_id)]\n",
    "            current_df_test = current_df[(current_df.userID == user_id)]\n",
    "\n",
    "            predictors_train = np.array(current_df_train[list_of_predictors].values)\n",
    "            predictors_test = np.array(current_df_test[list_of_predictors].values)\n",
    "            X_train_scaled = predictors_train #Only extra trees is currently being used!\n",
    "            X_test_scaled = predictors_test\n",
    "            answer_train = np.array(current_df_train['choice'])\n",
    "            answer_test = np.array(current_df_test['choice'])\n",
    "\n",
    "            if index_df == 0:#Selecting classifiers\n",
    "                clf = classifiers_agrad[0]\n",
    "            else:\n",
    "                clf = classifiers_seg[0]\n",
    "\n",
    "            #Fitting\n",
    "            clf.fit(X_train_scaled, answer_train)\n",
    "\n",
    "            #Testing!\n",
    "            for index_answer in range(0, len(answer_test)):\n",
    "                explanation = explainClassification(list_of_predictors, current_df['choice'].unique(),  \n",
    "                                                     X_train_scaled, X_test_scaled, clf, index_answer)\n",
    "\n",
    "                #Checking if prediction was correct!\n",
    "                current_answer = answer_test[index_answer]\n",
    "                predicted_answer = 0\n",
    "                predicted_answer_prob = 0\n",
    "                for index_exp in range(0, len(explanation.class_names)):#Most probable answer\n",
    "                        if explanation.predict_proba[index_exp] > predicted_answer_prob:\n",
    "                            predicted_answer_prob = explanation.predict_proba[index_exp]\n",
    "                            predicted_answer = explanation.class_names[index_exp]\n",
    "                #If answer was correct consider features relevances\n",
    "                #print(\"CORRECT \" + str(current_answer) + \" \" + str(predicted_answer) + \" \" + str(current_answer == predicted_answer))\n",
    "                if int(current_answer) == int(predicted_answer):\n",
    "                    #print(\"ENTREI\")\n",
    "                    exp_map = explanation.as_map() \n",
    "                    values = exp_map[exp_map.keys()[0]]\n",
    "                    for value in values:\n",
    "                        if value[0] in relevance_map.keys():\n",
    "                            relevance_map[value[0]].append(value[1])\n",
    "                        else:\n",
    "                            relevance_map[value[0]] = [value[1]]\n",
    "                    for index_class in range(0, len(explanation.class_names)):\n",
    "                        if explanation.class_names[index_class] in probabilities_map.keys():\n",
    "                            probabilities_map[explanation.class_names[index_class]].append(explanation.predict_proba[index_class])\n",
    "                        else:\n",
    "                            probabilities_map[explanation.class_names[index_class]] = [explanation.predict_proba[index_class]]\n",
    "\n",
    "        #Printing statistics for data frame being evaluated\n",
    "        #print(\"MAP: \" + str(relevance_map))\n",
    "        for key, value in relevance_map.iteritems(): \n",
    "            mean = np.mean(value)\n",
    "            std = np.std(value)\n",
    "            print( str(key) + \"\\t\" + list_of_predictors[key] + \"\\t\" + str(mean) + \"\\t\" + str(std))\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.1,  0.2,  0.3,  0.4,  0.5,  0.6,  0.7,  0.8,  0.9])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.3"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
